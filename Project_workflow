âœ… Step-by-Step Project Workflow
ğŸ”¹ STEP 1: Prepare Your Sample Data
Create a file named beverage_sales.csv

Use the sample data we discussed

Save it locally and/or upload to Azure Blob Storage container (e.g., sales_data)

ğŸ”¹ STEP 2: Set Up Azure SQL Database
Go to Azure Portal

Create an Azure SQL Server and SQL Database

Connect using SQL Server Management Studio (SSMS)

Create table with:

sql
Copy
Edit
CREATE TABLE BeverageSales (
    sale_id INT PRIMARY KEY,
    store_id INT,
    product_name NVARCHAR(100),
    category NVARCHAR(50),
    quantity_sold INT,
    unit_price DECIMAL(5,2),
    total_sale DECIMAL(6,2),
    sale_timestamp DATETIME,
    sale_date AS CAST(sale_timestamp AS DATE) PERSISTED,
    sale_hour AS DATEPART(HOUR, sale_timestamp) PERSISTED
);
ğŸ”¹ STEP 3: Upload CSV to Blob Storage
Create a Storage Account in Azure

Create a container (e.g., sales_data)

Upload beverage_sales.csv to this container

ğŸ”¹ STEP 4: Create Azure Data Factory Pipeline
In Azure Portal, create an Azure Data Factory

Create Linked Services:

Azure Blob Storage (source)

Azure SQL Database (sink)

Create Datasets:

Source: CSV in Blob

Sink: Azure SQL table

Create a Copy Data Activity pipeline

Map columns from CSV to table

Add an event-based trigger:

Trigger pipeline when a file is added to sales_data container

ğŸ”¹ STEP 5: Test the ETL Process
Upload a new CSV file to the Blob container

Watch the pipeline trigger and run

Confirm records in BeverageSales table

ğŸ”¹ STEP 6: Document the Project for GitHub
Create the following files:

ğŸ“ Folder Structure:
bash
Copy
Edit
/beverage-sales-etl
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ beverage_sales.csv
â”œâ”€â”€ sql_create_table.sql
â”œâ”€â”€ /screenshots/
â”‚   â”œâ”€â”€ pipeline_design.png
â”‚   â””â”€â”€ blob_upload.png
